# Chat model
# model-name is BUILD_AND_RUN_TIME_FIXED in the extension baked into native binary.
# Use cortex-m.ollama.model-name instead for true runtime config (incl. native).
de.u_project.cortex-m.ollama.model-name=${OLLAMA_MODEL_NAME:qwen3:14b}
de.u_project.cortext-m.ollama.api-key=${API_KEY:dummy}
quarkus.langchain4j.ollama.chat-model.model-name=${de.u_project.cortex-m.ollama.model-name}
quarkus.langchain4j.ollama.chat-model.temperature=1
quarkus.langchain4j.timeout=180s
quarkus.langchain4j.embedding-model.provider=ollama
quarkus.langchain4j.chat-model.provider=ollama
%dev.quarkus.langchain4j.ollama.base-url=http://localhost:11434
# Embedding model
quarkus.langchain4j.ollama.embedding-model.model-name=snowflake-arctic-embed:latest
# Vector store
quarkus.langchain4j.pgvector.dimension=1024
#
quarkus.http.root-path=/api/cortex-m/v1
%dev.quarkus.langchain4j.log-requests=true
# Scheduler
quarkus.scheduler.start-mode=forced